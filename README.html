<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><style>body {
  max-width: 980px;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 45px;
}

@font-face {
  font-family: fontawesome-mini;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body b,
.markdown-body strong {
  font-weight: bold;
}

.markdown-body mark {
  background: #ff0;
  color: #000;
  font-style: italic;
  font-weight: bold;
}

.markdown-body sub,
.markdown-body sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
.markdown-body sup {
  top: -0.5em;
}
.markdown-body sub {
  bottom: -0.25em;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre,
.markdown-body samp {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body .codehilitetable {
  border: 0;
  border-spacing: 0;
}

.markdown-body .codehilitetable tr {
  border: 0;
}

.markdown-body .codehilitetable pre,
.markdown-body .codehilitetable div.codehilite {
  margin: 0;
}

.markdown-body .linenos,
.markdown-body .code,
.markdown-body .codehilitetable td {
  border: 0;
  padding: 0;
}

.markdown-body td:not(.linenos) .linenodiv {
  padding: 0 !important;
}

.markdown-body .code {
  width: 100%;
}

.markdown-body .linenos div pre,
.markdown-body .linenodiv pre,
.markdown-body .linenodiv {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-left-radius: 3px;
  -webkit-border-bottom-left-radius: 3px;
  -moz-border-radius-topleft: 3px;
  -moz-border-radius-bottomleft: 3px;
  border-top-left-radius: 3px;
  border-bottom-left-radius: 3px;
}

.markdown-body .code div pre,
.markdown-body .code div {
  border: 0;
  -webkit-border-radius: 0;
  -moz-border-radius: 0;
  border-radius: 0;
  -webkit-border-top-right-radius: 3px;
  -webkit-border-bottom-right-radius: 3px;
  -moz-border-radius-topright: 3px;
  -moz-border-radius-bottomright: 3px;
  border-top-right-radius: 3px;
  border-bottom-right-radius: 3px;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
  line-height: 1.4;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before,
.markdown-body hr:after {
  display: table;
  content: " ";
}

.markdown-body hr:after {
  clear: both;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code,
.markdown-body pre,
.markdown-body samp {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -moz-linear-gradient(#fefefe, #e7e7e7);
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .headerlink {
  font: normal 400 16px fontawesome-mini;
  vertical-align: middle;
  margin-left: -16px;
  float: left;
  display: inline-block;
  text-decoration: none;
  opacity: 0;
  color: #333;
}

.markdown-body .headerlink:focus {
  outline: none;
}

.markdown-body h1 .headerlink {
  margin-top: 0.8rem;
}

.markdown-body h2 .headerlink,
.markdown-body h3 .headerlink {
  margin-top: 0.6rem;
}

.markdown-body h4 .headerlink {
  margin-top: 0.2rem;
}

.markdown-body h5 .headerlink,
.markdown-body h6 .headerlink {
  margin-top: 0;
}

.markdown-body .headerlink:hover,
.markdown-body h1:hover .headerlink,
.markdown-body h2:hover .headerlink,
.markdown-body h3:hover .headerlink,
.markdown-body h4:hover .headerlink,
.markdown-body h5:hover .headerlink,
.markdown-body h6:hover .headerlink {
  opacity: 1;
  text-decoration: none;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre,
.markdown-body .admonition {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code,
.markdown-body samp {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .codehilite {
  margin-bottom: 16px;
}

.markdown-body .codehilite pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .codehilite pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

/* Admonition */
.markdown-body .admonition {
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  position: relative;
  border-radius: 3px;
  border: 1px solid #e0e0e0;
  border-left: 6px solid #333;
  padding: 10px 10px 10px 30px;
}

.markdown-body .admonition table {
  color: #333;
}

.markdown-body .admonition p {
  padding: 0;
}

.markdown-body .admonition-title {
  font-weight: bold;
  margin: 0;
}

.markdown-body .admonition>.admonition-title {
  color: #333;
}

.markdown-body .attention>.admonition-title {
  color: #a6d796;
}

.markdown-body .caution>.admonition-title {
  color: #d7a796;
}

.markdown-body .hint>.admonition-title {
  color: #96c6d7;
}

.markdown-body .danger>.admonition-title {
  color: #c25f77;
}

.markdown-body .question>.admonition-title {
  color: #96a6d7;
}

.markdown-body .note>.admonition-title {
  color: #d7c896;
}

.markdown-body .admonition:before,
.markdown-body .attention:before,
.markdown-body .caution:before,
.markdown-body .hint:before,
.markdown-body .danger:before,
.markdown-body .question:before,
.markdown-body .note:before {
  font: normal normal 16px fontawesome-mini;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  line-height: 1.5;
  color: #333;
  position: absolute;
  left: 0;
  top: 0;
  padding-top: 10px;
  padding-left: 10px;
}

.markdown-body .admonition:before {
  content: "\f056\00a0";
  color: 333;
}

.markdown-body .attention:before {
  content: "\f058\00a0";
  color: #a6d796;
}

.markdown-body .caution:before {
  content: "\f06a\00a0";
  color: #d7a796;
}

.markdown-body .hint:before {
  content: "\f05a\00a0";
  color: #96c6d7;
}

.markdown-body .danger:before {
  content: "\f057\00a0";
  color: #c25f77;
}

.markdown-body .question:before {
  content: "\f059\00a0";
  color: #96a6d7;
}

.markdown-body .note:before {
  content: "\f040\00a0";
  color: #d7c896;
}

.markdown-body .admonition::after {
  content: normal;
}

.markdown-body .attention {
  border-left: 6px solid #a6d796;
}

.markdown-body .caution {
  border-left: 6px solid #d7a796;
}

.markdown-body .hint {
  border-left: 6px solid #96c6d7;
}

.markdown-body .danger {
  border-left: 6px solid #c25f77;
}

.markdown-body .question {
  border-left: 6px solid #96a6d7;
}

.markdown-body .note {
  border-left: 6px solid #d7c896;
}

.markdown-body .admonition>*:first-child {
  margin-top: 0 !important;
}

.markdown-body .admonition>*:last-child {
  margin-bottom: 0 !important;
}

/* progress bar*/
.markdown-body .progress {
  display: block;
  width: 300px;
  margin: 10px 0;
  height: 24px;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #ededed;
  position: relative;
  box-shadow: inset -1px 1px 3px rgba(0, 0, 0, .1);
}

.markdown-body .progress-label {
  position: absolute;
  text-align: center;
  font-weight: bold;
  width: 100%; margin: 0;
  line-height: 24px;
  color: #333;
  text-shadow: 1px 1px 0 #fefefe, -1px -1px 0 #fefefe, -1px 1px 0 #fefefe, 1px -1px 0 #fefefe, 0 1px 0 #fefefe, 0 -1px 0 #fefefe, 1px 0 0 #fefefe, -1px 0 0 #fefefe, 1px 1px 2px #000;
  -webkit-font-smoothing: antialiased !important;
  white-space: nowrap;
  overflow: hidden;
}

.markdown-body .progress-bar {
  height: 24px;
  float: left;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
  background-color: #96c6d7;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
  background-size: 30px 30px;
  background-image: -webkit-linear-gradient(
    135deg, rgba(255, 255, 255, .4) 27%,
    transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%,
    transparent 77%, transparent
  );
  background-image: -moz-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -ms-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: -o-linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
  background-image: linear-gradient(
    135deg,
    rgba(255, 255, 255, .4) 27%, transparent 27%,
    transparent 52%, rgba(255, 255, 255, .4) 52%,
    rgba(255, 255, 255, .4) 77%, transparent 77%,
    transparent
  );
}

.markdown-body .progress-100plus .progress-bar {
  background-color: #a6d796;
}

.markdown-body .progress-80plus .progress-bar {
  background-color: #c6d796;
}

.markdown-body .progress-60plus .progress-bar {
  background-color: #d7c896;
}

.markdown-body .progress-40plus .progress-bar {
  background-color: #d7a796;
}

.markdown-body .progress-20plus .progress-bar {
  background-color: #d796a6;
}

.markdown-body .progress-0plus .progress-bar {
  background-color: #c25f77;
}

.markdown-body .candystripe-animate .progress-bar{
  -webkit-animation: animate-stripes 3s linear infinite;
  -moz-animation: animate-stripes 3s linear infinite;
  animation: animate-stripes 3s linear infinite;
}

@-webkit-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@-moz-keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

@keyframes animate-stripes {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 60px 0;
  }
}

.markdown-body .gloss .progress-bar {
  box-shadow:
    inset 0 4px 12px rgba(255, 255, 255, .7),
    inset 0 -12px 0 rgba(0, 0, 0, .05);
}

/* MultiMarkdown Critic Blocks */
.markdown-body .critic_mark {
  background: #ff0;
}

.markdown-body .critic_delete {
  color: #c82829;
  text-decoration: line-through;
}

.markdown-body .critic_insert {
  color: #718c00 ;
  text-decoration: underline;
}

.markdown-body .critic_comment {
  color: #8e908c;
  font-style: italic;
}

.markdown-body .headeranchor {
  font: normal normal 16px fontawesome-mini;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.headeranchor:before {
  content: '\e157';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 4px 0.25em -20px;
  vertical-align: middle;
}

/* Media */
@media only screen and (min-width: 480px) {
  .markdown-body {
    font-size:14px;
  }
}

@media only screen and (min-width: 768px) {
  .markdown-body {
    font-size:16px;
  }
}

@media print {
  .markdown-body * {
    background: transparent !important;
    color: black !important;
    filter:none !important;
    -ms-filter: none !important;
  }

  .markdown-body {
    font-size:12pt;
    max-width:100%;
    outline:none;
    border: 0;
  }

  .markdown-body a,
  .markdown-body a:visited {
    text-decoration: underline;
  }

  .markdown-body .headeranchor-link {
    display: none;
  }

  .markdown-body a[href]:after {
    content: " (" attr(href) ")";
  }

  .markdown-body abbr[title]:after {
    content: " (" attr(title) ")";
  }

  .markdown-body .ir a:after,
  .markdown-body a[href^="javascript:"]:after,
  .markdown-body a[href^="#"]:after {
    content: "";
  }

  .markdown-body pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .markdown-body pre,
  .markdown-body blockquote {
    border: 1px solid #999;
    padding-right: 1em;
    page-break-inside: avoid;
  }

  .markdown-body .progress,
  .markdown-body .progress-bar {
    -moz-box-shadow: none;
    -webkit-box-shadow: none;
    box-shadow: none;
  }

  .markdown-body .progress {
    border: 1px solid #ddd;
  }

  .markdown-body .progress-bar {
    height: 22px;
    border-right: 1px solid #ddd;
  }

  .markdown-body tr,
  .markdown-body img {
    page-break-inside: avoid;
  }

  .markdown-body img {
    max-width: 100% !important;
  }

  .markdown-body p,
  .markdown-body h2,
  .markdown-body h3 {
    orphans: 3;
    widows: 3;
  }

  .markdown-body h2,
  .markdown-body h3 {
    page-break-after: avoid;
  }
}
</style><style>/*GitHub*/
.codehilite {background-color:#fff;color:#333333;}
.codehilite .hll {background-color:#ffffcc;}
.codehilite .c{color:#999988;font-style:italic}
.codehilite .err{color:#a61717;background-color:#e3d2d2}
.codehilite .k{font-weight:bold}
.codehilite .o{font-weight:bold}
.codehilite .cm{color:#999988;font-style:italic}
.codehilite .cp{color:#999999;font-weight:bold}
.codehilite .c1{color:#999988;font-style:italic}
.codehilite .cs{color:#999999;font-weight:bold;font-style:italic}
.codehilite .gd{color:#000000;background-color:#ffdddd}
.codehilite .ge{font-style:italic}
.codehilite .gr{color:#aa0000}
.codehilite .gh{color:#999999}
.codehilite .gi{color:#000000;background-color:#ddffdd}
.codehilite .go{color:#888888}
.codehilite .gp{color:#555555}
.codehilite .gs{font-weight:bold}
.codehilite .gu{color:#800080;font-weight:bold}
.codehilite .gt{color:#aa0000}
.codehilite .kc{font-weight:bold}
.codehilite .kd{font-weight:bold}
.codehilite .kn{font-weight:bold}
.codehilite .kp{font-weight:bold}
.codehilite .kr{font-weight:bold}
.codehilite .kt{color:#445588;font-weight:bold}
.codehilite .m{color:#009999}
.codehilite .s{color:#dd1144}
.codehilite .n{color:#333333}
.codehilite .na{color:teal}
.codehilite .nb{color:#0086b3}
.codehilite .nc{color:#445588;font-weight:bold}
.codehilite .no{color:teal}
.codehilite .ni{color:purple}
.codehilite .ne{color:#990000;font-weight:bold}
.codehilite .nf{color:#990000;font-weight:bold}
.codehilite .nn{color:#555555}
.codehilite .nt{color:navy}
.codehilite .nv{color:teal}
.codehilite .ow{font-weight:bold}
.codehilite .w{color:#bbbbbb}
.codehilite .mf{color:#009999}
.codehilite .mh{color:#009999}
.codehilite .mi{color:#009999}
.codehilite .mo{color:#009999}
.codehilite .sb{color:#dd1144}
.codehilite .sc{color:#dd1144}
.codehilite .sd{color:#dd1144}
.codehilite .s2{color:#dd1144}
.codehilite .se{color:#dd1144}
.codehilite .sh{color:#dd1144}
.codehilite .si{color:#dd1144}
.codehilite .sx{color:#dd1144}
.codehilite .sr{color:#009926}
.codehilite .s1{color:#dd1144}
.codehilite .ss{color:#990073}
.codehilite .bp{color:#999999}
.codehilite .vc{color:teal}
.codehilite .vg{color:teal}
.codehilite .vi{color:teal}
.codehilite .il{color:#009999}
.codehilite .gc{color:#999;background-color:#EAF2F5}
</style><title>README</title></head><body><article class="markdown-body"><h1 id="conditional-adversarial-camera-model-anonymization">Conditional Adversarial Camera Model Anonymization<a class="headerlink" href="#conditional-adversarial-camera-model-anonymization" title="Permanent link"></a></h1>
<p>PyTorch implementation of <a href="https://arxiv.org/abs/2002.07798">Conditional Adversarial Camera Model Anonymization</a> (Proceedings of the European Conference on Computer Vision (ECCV) 2020 Advances in Image Manipulation Workshop).</p>
<h2 id="contents">Contents<a class="headerlink" href="#contents" title="Permanent link"></a></h2>
<ul>
<li><a href="#problem-description">Problem description</a></li>
<li><a href="#cama-architecture-description">Cama architecture description</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#dataset">Dataset</a></li>
<li><a href="#training">Training</a></li>
<li><a href="#testing">Testing</a></li>
<li><a href="#references">References</a></li>
</ul>
<h3 id="problem-description">Problem description<a class="headerlink" href="#problem-description" title="Permanent link"></a></h3>
<h4 id="camera-model-attribution">Camera model attribution<a class="headerlink" href="#camera-model-attribution" title="Permanent link"></a></h4>
<p>Digital photographs can be <em>blindly</em> attributed to the specific camera model used for capture.</p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/blind-att.png"  /></p>

<h4 id="camera-model-anonymization">Camera model anonymization<a class="headerlink" href="#camera-model-anonymization" title="Permanent link"></a></h4>
<p>Conditional Adversarial Camera Model Anonymization (Cama) offers a way to preserve privacy by transforming these artifacts such that the apparent capture model is changed (targeted transformation). That is, given an image and a target label condition, the applied transformation causes a non-interactive black-box <em>target</em> (i.e. to be attacked/fooled) convnet classifier <img src="https://render.githubusercontent.com/render/math?math=\large F"> to predict the target label given the transformed image. While at the same time retaining the original image content.</p>
<p>However, Cama is trained in a <em>non-interactive black-box setting</em>: Cama does not have knowledge of the parameters, architecture or training randomness of <img src="https://render.githubusercontent.com/render/math?math=\large F">, nor can Cama interact with it. </p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/cam-anon.png" /></p>

<h4 id="anonymizing-in-distribution-images">Anonymizing in-distribution images<a class="headerlink" href="#anonymizing-in-distribution-images" title="Permanent link"></a></h4>
<p>Cama is able to successfully perform targeted transformations on in-distribution images (i.e. images captured by camera models known to it).</p>
<p>Example (below) of Cama transformed images <img src="https://render.githubusercontent.com/render/math?math=\large x^'"> with different target label conditions <img src="https://render.githubusercontent.com/render/math?math=\large y^'"> given an in-distribution input image <img src="https://render.githubusercontent.com/render/math?math=\large x"> (whose ground truth label is <img src="https://render.githubusercontent.com/render/math?math=\large y">). The applied transformations (amplified for visualization purposes) are shown as <img src="https://render.githubusercontent.com/render/math?math=\large \delta">.</p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/flower.png" width="700" /></p>

<h4 id="anonymizing-out-of-distribution-images">Anonymizing out-of-distribution images<a class="headerlink" href="#anonymizing-out-of-distribution-images" title="Permanent link"></a></h4>
<p>Cama is also able to successfully perform targeted transformations on out-of-distribution images (i.e. images captured by camera models unknown to it).</p>
<p>Example (below) of Cama transformed images <img src="https://render.githubusercontent.com/render/math?math=\large x^'"> with different target label conditions <img src="https://render.githubusercontent.com/render/math?math=\large y^'"> given an out-of-distribution input image <img src="https://render.githubusercontent.com/render/math?math=\large x"> (whose ground truth label is <img src="https://render.githubusercontent.com/render/math?math=\large y">). The applied transformations (amplified for visualization purposes) are shown as <img src="https://render.githubusercontent.com/render/math?math=\large \delta">.</p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/building.png" /></p>

<h4 id="previous-approaches-to-camera-model-anonymization">Previous approaches to camera model anonymization<a class="headerlink" href="#previous-approaches-to-camera-model-anonymization" title="Permanent link"></a></h4>
<p>Previous works view anonymisation as requiring the transformation of pixel non-uniformity (PNU) noise, which is defined as slight variations in the sensitivity of individual pixel sensors. Although initially device-specific, these variations propagate nonlinearly through a digital camera&rsquo;s processing steps that result in the viewable image, and thus end up also depending on model-specific aspects.</p>
<p>Model anonymization approaches based on PNU suppress the image content, using a denoising filter, and instead work with the noise residual, i.e. the observed image minus the estimated noise-free image content.
This is premised on improving the signal-to-noise ratio between the model-specific artifacts, i.e. the signal of interest, and the observable image. However, this precludes any anonymization process from attending to discriminative low spatial frequency model-specific artifacts, since they no longer exist within the high-frequency noise residual. </p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/previous-works.png" /></p>

<h3 id="cama-architecture-description">Cama architecture description<a class="headerlink" href="#cama-architecture-description" title="Permanent link"></a></h3>
<p>We denote by <img src="https://render.githubusercontent.com/render/math?math=\large x\in\mathbb{R}^d"> and <img src="https://render.githubusercontent.com/render/math?math=\large y\in\mathbb{N}_c=\{1,\dots,c\}"> an image and its ground truth (source) camera model label, respectively, sampled from a dataset <img src="https://render.githubusercontent.com/render/math?math=\large p_{\text{data}}">. Consider a <em>target</em> (i.e. to be attacked) convnet classifier <img src="https://render.githubusercontent.com/render/math?math=\large F"> with <img src="https://render.githubusercontent.com/render/math?math=\large c"> classes trained over input-output tuples <img src="https://render.githubusercontent.com/render/math?math=\large (x,y)\sim p_{\mathrm{data}}(x,y)">. Given <img src="https://render.githubusercontent.com/render/math?math=\large x">, <img src="https://render.githubusercontent.com/render/math?math=\large F"> outputs a prediction vector of class probabilities <img src="https://render.githubusercontent.com/render/math?math=\large F:x\mapsto F(x)\in[0,1]^{c}">.</p>
<p>As aforementioned, Cama operates in a <em>non-interactive black-box setting</em>. We do, however, assume that Cama can sample from a dataset similar to <img src="https://render.githubusercontent.com/render/math?math=\large p_{\mathrm{data}}">, which we denote by <img src="https://render.githubusercontent.com/render/math?math=\large q_{\mathrm{data}}">. Precisely, Cama can sample tuples of the following form: <img src="https://render.githubusercontent.com/render/math?math=\large (x,y)\sim q_{\text{data}}(x,y)"> s.t. <img src="https://render.githubusercontent.com/render/math?math=\large y\in\mathbb{N}_{c^'}">, where <img src="https://render.githubusercontent.com/render/math?math=\large (x,y)\sim q_{\text{data}}(x,y)"> s.t. <img src="https://render.githubusercontent.com/render/math?math=\large c^' \leq c">. That is, the set of possible image class labels in <img src="https://render.githubusercontent.com/render/math?math=\large p_{\text{data}}"> is a superset of the set of possible image class labels in <img src="https://render.githubusercontent.com/render/math?math=\large q_{\text{data}}">, i.e. <img src="https://render.githubusercontent.com/render/math?math=\large \mathbb{N}_{c}\supseteq \mathbb{N}_{c'}">.</p>
<p>Suppose <img src="https://render.githubusercontent.com/render/math?math=\large (x,y)\sim q_{\text{data}}(x,y)"> and <img src="https://render.githubusercontent.com/render/math?math=\large y^' \in\mathbb{N}_{c^'}">, where <img src="https://render.githubusercontent.com/render/math?math=\large y^' \neq y"> is a target label. Our aim is to learn a function <img src="https://render.githubusercontent.com/render/math?math=\large G:(x,y^')\mapsto x^' \approx x"> s.t. the maximum probability satisfies <img src="https://render.githubusercontent.com/render/math?math=\large \argmax_{i} F(x^')_i=y^'">. This is known as a <em>targeted</em> attack, whereas the maximum probability of an <em>untargeted</em> attack must satisfy <img src="https://render.githubusercontent.com/render/math?math=\large \arg \max_{i} F(x^')_i\neq y">. <em>Cama performs targeted attacks</em>.</p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/model.png" /></p>

<p>Cama has two class conditional components: a generator <img src="https://render.githubusercontent.com/render/math?math=\large G"> that transforms an image <img src="https://render.githubusercontent.com/render/math?math=\large x"> conditioned on a target class label <img src="https://render.githubusercontent.com/render/math?math=\large y^'">, and a discriminator <img src="https://render.githubusercontent.com/render/math?math=\large D"> that predicts whether the low-level high-frequency pixel value dependency features of any given image conditioned on a label are real or fake. In addition, Cama has a fixed (w.r.t. its parameters) dual-stream discriminative decision-making component <img src="https://render.githubusercontent.com/render/math?math=\large E"> (evaluator) that decides whether a transformed image <img src="https://render.githubusercontent.com/render/math?math=\large x"> belongs to its target class <img src="https://render.githubusercontent.com/render/math?math=\large y">. In essence, <img src="https://render.githubusercontent.com/render/math?math=\large E"> serves as a surrogate for the non-interactive black-box <img src="https://render.githubusercontent.com/render/math?math=\large F">. W.r.t. <img src="https://render.githubusercontent.com/render/math?math=\large E">, a transformed image <img src="https://render.githubusercontent.com/render/math?math=\large x^'"> is decomposed into its high and low spatial frequency components (<img src="https://render.githubusercontent.com/render/math?math=\large x^'_\text{H}"> and <img src="https://render.githubusercontent.com/render/math?math=\large x^'_\text{L}">, respectively), via <img src="https://render.githubusercontent.com/render/math?math=\large E_0">, with each assigned to a separate stream (<img src="https://render.githubusercontent.com/render/math?math=\large E_\text{H}"> and <img src="https://render.githubusercontent.com/render/math?math=\large E_\text{L}">, respectively). The evaluator then reasons over the information present in <img src="https://render.githubusercontent.com/render/math?math=\large x^'_\text{H}"> and <img src="https://render.githubusercontent.com/render/math?math=\large x^'_\text{L}"> separately (via <img src="https://render.githubusercontent.com/render/math?math=\large E_\text{H}"> and <img src="https://render.githubusercontent.com/render/math?math=\large E_\text{L}">, respectively). This reinforces the transformation process, as <img src="https://render.githubusercontent.com/render/math?math=\large G"> is constrained to transform both high and low spatial frequency camera model-specific artifacts used by the evaluator for discrimination.</p>
<p>During conditional adversarial training, <img src="https://render.githubusercontent.com/render/math?math=\large G"> minimizes:</p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/g-minimizes.png" width="700" /></p>

<p>whereas <img src="https://render.githubusercontent.com/render/math?math=\large D"> minimizes:</p>
<p align="center"><img src="/Users/jerone/Desktop/cama/images/d-minimizes.png" width="700" /></p>

<h2 id="dependencies">Dependencies<a class="headerlink" href="#dependencies" title="Permanent link"></a></h2>
<ul>
<li>Python 3</li>
<li>CUDA</li>
<li><a href="http://pytorch.org/">PyTorch</a> (1.6.0), <a href="https://github.com/pytorch/vision/">torchvision</a> (0.7.0), <a href="https://github.com/sksq96/pytorch-summary">torchsummary</a> (1.5.1), <a href="https://www.scipy.org">SciPy</a> (1.4.1), <a href="https://pywavelets.readthedocs.io/en/latest/">PyWavelets</a> (1.1.1), <a href="https://scikit-image.org/">scikit-image</a> (0.16.2), <a href="https://pandas.pydata.org">pandas</a> (1.1.3), <a href="https://requests.readthedocs.io/en/master/">requests</a> (2.24.0), <a href="https://matplotlib.org/">matplotlib</a> (3.2.1), <a href="https://numpy.org/">NumPy</a> (1.17.4), <a href="https://github.com/tqdm/tqdm/">tqdm</a> (4.45.0), <a href="https://pypi.org/project/colour-demosaicing/">colour-demosaicing</a> (0.1.5), <a href="https://pillow.readthedocs.io/en/stable/">Pillow</a> (7.2.0)</li>
</ul>
<h2 id="installation">Installation<a class="headerlink" href="#installation" title="Permanent link"></a></h2>
<p>Simply clone this repository:</p>
<div class="codehilite"><pre>git clone https://github.com/jeroneandrews/cama.git
<span class="nb">cd </span>cama
pip install -r requirements.txt
</pre></div>

<h2 id="dataset">Dataset<a class="headerlink" href="#dataset" title="Permanent link"></a></h2>
<p>To download and preprocess the <a href="http://forensics.inf.tu-dresden.de/ddimgdb/">Dresden image database</a> of JPEG colour images captured by 27 unique camera models run:</p>
<p><div class="codehilite"><pre>./download.sh
</pre></div>
(This will also clone the <a href="https://github.com/richzhang/PerceptualSimilarity">Perceptual Similarity repository</a>, which is used for computing the distortion introduced by the camera model anonymization transformation process).</p>
<p>The preprocessed data samples will be saved to the following folders as <code>.pth</code> files:
<div class="codehilite"><pre>data/dataset/dresden_preprocessed/image    # RGB images [4.8GB]
data/dataset/dresden_preprocessed/prnu     # prnus with linear patterns [20GB]
data/dataset/dresden_preprocessed/remosaic # remosaiced RGB images [39GB]
</pre></div>
Each sample (<code>.pth</code> file) is a tensor of size <code>(3, 512, 512)</code>. Note that you can change the image size (as well as other parameters) in <code>data/config.py</code> file.</p>
<p>Each of the above folders (e.g. <code>data/dataset/dresden_preprocessed/image</code>) contain the following subfolders:</p>
<ul>
<li><code>adversary</code> (Cama train/validation data)</li>
<li><code>examiner</code> (non-interactive black-box target classifier train/validation data)</li>
<li><code>test</code> (in-distribution test data)</li>
<li><code>test_outdist</code> (out-of-distribution test data)</li>
</ul>
<p>(Note that the folder <code>data/dataset/dresden_preprocessed/remosaic</code> does not have a subfolder <code>remosaic</code>, as this type of data is never used by a non-interactive black-box classifier).</p>
<p>The number of unique samples in <code>adversary</code>, <code>examiner</code>, <code>test</code> and <code>test_outdist</code> should be 2508, 2463, 600 and 600, respectively. </p>
<h2 id="training">Training<a class="headerlink" href="#training" title="Permanent link"></a></h2>
<p>To reproduce the results in the paper, run the following scripts:</p>
<div class="codehilite"><pre>./scripts/estimator.sh
./scripts/classifier.sh
./scripts/anonymize.sh
</pre></div>

<h3 id="train-a-prnu-estimator">Train a PRNU estimator<a class="headerlink" href="#train-a-prnu-estimator" title="Permanent link"></a></h3>
<p>To train Cama you must first train a PRNU estimator, since it is required for the adversarial training process. (Specifically, it is required by the adversarial evaluator). To train an estimator run <code>python estimator.py</code>. See below for a full list of possible parameters:</p>
<div class="codehilite"><pre>python estimator.py

<span class="c1"># main parameters</span>
--user adversary                                <span class="c1"># Dataset (adversary / examiner)</span>
--ptc_sz <span class="m">64</span>                                     <span class="c1"># Patch width/height</span>
--ptc_fm <span class="m">3</span>                                      <span class="c1"># Number of input feature maps (channels)</span>
--expanded_cms False                            <span class="c1"># Training with an expanded set of camera models (only valid if user is examiner)</span>

<span class="c1"># network architecture</span>
--estimator_output prnu_lp                      <span class="c1"># Estimate prnu_lp, i.e. prnu noise with linear pattern, of an rgb image (prnu_lp)</span>
--nef <span class="m">64</span>                                        <span class="c1"># Number of feature maps in the first and last convolutional layers</span>
--n_blocks <span class="m">2</span>                                    <span class="c1"># Number of residual blocks in the estimator</span>
--drop_rate <span class="m">0</span>                                   <span class="c1"># Dropout rate in the residual blocks</span>

<span class="c1"># training parameters</span>
--batch_size <span class="m">128</span>                                <span class="c1"># Batch size (training)</span>
--test_batch_size <span class="m">32</span>                            <span class="c1"># Batch size (validation / testing)&quot;)</span>
--rnd_crops False                               <span class="c1"># Extract patches randomly (True) or from a non-overlapping grid (False)</span>
--n_epochs <span class="m">90</span>                                   <span class="c1"># Total number of epochs</span>
--n_samples_per_epoch <span class="m">150000</span>                    <span class="c1"># Number of training samples per epoch</span>
--optimizer adam_standard,weight_decay<span class="o">=</span>0.0005   <span class="c1"># Estimator optimizer (adam_standard,weight_decay=0.0005 / sgd,lr=0.1,weight_decay=0.0005,momentum=0.9,nesterov=True)</span>
--save_opt True                                 <span class="c1"># Save optimizer</span>
--lr_milestones <span class="o">[]</span>                              <span class="c1"># Epochs to divide learning rate by 10</span>

<span class="c1"># loaders / gpus</span>
--n_workers <span class="m">10</span>                                  <span class="c1"># Number of workers per data loader</span>
--pin_memory True                               <span class="c1"># Pin memory of data loaders</span>
--gpu_devices <span class="m">0</span>                                 <span class="c1"># Which gpu devices to use</span>

<span class="c1"># reload</span>
--reload <span class="s2">&quot;&quot;</span>                                     <span class="c1"># Path to a pre-trained estimator (and optimizer if saved)</span>
--resume False                                  <span class="c1"># Resume training</span>

<span class="c1"># debug</span>
--debug False                                   <span class="c1"># Debug mode (only use a subset of the available data)</span>
</pre></div>

<h3 id="train-an-evaluator">Train an evaluator<a class="headerlink" href="#train-an-evaluator" title="Permanent link"></a></h3>
<p>Cama requires a dual-stream evaluator. Each stream is trained seperately. To train Cama&rsquo;s high-frequency evaluator stream run <code>python classifier.py --clf_input prnu_lp --est_reload [PATH_TO_PRNU_ESTIMATOR.pth]</code>.  To train Cama&rsquo;s low-frequency evaluator stream run <code>python classifier.py --clf_input prnu_lp_low --est_reload [PATH_TO_PRNU_ESTIMATOR.pth]</code>. See below for a full list of possible parameters:</p>
<div class="codehilite"><pre>python classifier.py

<span class="c1"># main parameters</span>
--user adversary                                <span class="c1"># Dataset (adversary / examiner)</span>
--ptc_sz <span class="m">64</span>                                     <span class="c1"># Patch width/height</span>
--ptc_fm <span class="m">3</span>                                      <span class="c1"># Number of input feature maps (channels)</span>
--expanded_cms False                            <span class="c1"># Training with an expanded set of camera models (only valid if user is examiner)</span>

<span class="c1"># network architecture</span>
--clf_input prnu_lp                             <span class="c1"># Classifier input (con_conv / finite_difference / fixed_hpf / rgb+con_conv / rgb+finite_difference / rgb+fixed_hpf / rgb / prnu_lp / prnu_lp_low)</span>
--clf_architecture resnet18                     <span class="c1"># Classifier architecture (vgg11 / vgg13 / vgg16 / vgg19 / resnet18 / resnet34 / resnet50 / densenet40 / densenet100)</span>
--drop_rate 0.5                                 <span class="c1"># Dropout in the classifier</span>
--efficient False                               <span class="c1"># Memory efficient (but slower) training for DenseNet models</span>

<span class="c1"># training parameters</span>
--batch_size <span class="m">128</span>                                <span class="c1"># Batch size (training)</span>
--test_batch_size <span class="m">16</span>                            <span class="c1"># Batch size (validation / testing)&quot;)</span>
--rnd_crops False                               <span class="c1"># Extract patches randomly (True) or from a non-overlapping grid (False)</span>
--n_epochs <span class="m">90</span>                                   <span class="c1"># Total number of epochs</span>
--n_samples_per_epoch <span class="m">150000</span>                    <span class="c1"># Number of training samples per epoch</span>
--optimizer sgd,lr<span class="o">=</span>0.1,weight_decay<span class="o">=</span>0.0005,<span class="se">\</span>
<span class="nv">momentum</span><span class="o">=</span>0.9,nesterov<span class="o">=</span>True                      <span class="c1"># Classifier optimizer (sgd,lr=0.1,weight_decay=0.0005,momentum=0.9,nesterov=True / adagrad,lr=0.1,lr_decay=0.05)</span>
--save_opt True                                 <span class="c1"># Save optimizer</span>
--lr_milestones <span class="m">45</span> <span class="m">68</span>                           <span class="c1"># Epochs to divide learning rate by 10</span>

<span class="c1"># loaders / gpus</span>
--n_workers <span class="m">10</span>                                  <span class="c1"># Number of workers per data loader</span>
--pin_memory True                               <span class="c1"># Pin memory of data loaders</span>
--gpu_devices <span class="m">0</span>                                 <span class="c1"># Which gpu devices to use</span>

<span class="c1"># reload</span>
--reload <span class="s2">&quot;&quot;</span>                                     <span class="c1"># Path to a pre-trained classifier (and optimizer if saved)</span>
--est_reload <span class="s2">&quot;&quot;</span>                                 <span class="c1"># Path to a a pre-trained PRNU estimator (trained with estimator.py)</span>
--resume False                                  <span class="c1"># Resume training</span>

<span class="c1"># debug</span>
--debug False                                   <span class="c1"># Debug mode (only use a subset of the available data)</span>
</pre></div>

<h3 id="train-cama">Train Cama<a class="headerlink" href="#train-cama" title="Permanent link"></a></h3>
<p>(Assuming you have trained a PRNU estimator, a high-frequency evaluator and alow-frequency evaluator). To train Cama run <code>python train.py --clf_low_reload [PATH_TO_LOW_FREQ_EVALUATOR.pth] --clf_high_reload PATH_TO_HIGH_FREQ_EVALUATOR --est_reload [PATH_TO_PRNU_ESTIMATOR.pth]</code>. See below for a full list of possible parameters:</p>
<div class="codehilite"><pre>python train.py

<span class="c1"># main parameters</span>
--ptc_sz <span class="m">64</span>                         <span class="c1"># Patch width/height</span>
--ptc_fm <span class="m">3</span>                          <span class="c1"># Number of input feature maps (channels)</span>

<span class="c1"># generator architecture</span>
--gen_input remosaic                <span class="c1"># Generator input (rgb / remosaic)</span>
--ngf <span class="m">64</span>                            <span class="c1"># Number of feature maps in the generator&#39;s first and last convolutional layers</span>
--n_blocks <span class="m">2</span>                        <span class="c1"># Number of residual blocks in the generator</span>

<span class="c1"># discriminator architecture</span>
--dis_input con_conv                <span class="c1"># Classifier input (prnu_lp / rgb / con_conv / prnu_lp_low+prnu_lp)</span>
--ndf <span class="m">64</span>                            <span class="c1"># Number of feature maps in the discriminator&#39;s first convolutional layer </span>
--use_lsgan True                    <span class="c1"># Least squares GAN</span>
--dis_type patch                    <span class="c1"># Discriminator type (patch / pixel)</span>
--n_dis_layers <span class="m">2</span>                    <span class="c1"># Number of discriminator layers (only used if the discriminator is a patch discriminator)</span>
--dis_drop_rate <span class="m">0</span>                   <span class="c1"># Dropout in the discriminator</span>

<span class="c1"># training parameters</span>
--pixel_loss l1                     <span class="c1"># Pixel-wise loss (l1 / l2)</span>
--pixel_loss_schedule 10.0, <span class="m">0</span>       <span class="c1"># First argument: pixel loss feedback coefficient (0 to disable). Second argument: epochs to progressively increase the pixel loss coefficient (0 to disable).</span>
--adv_loss_schedule 1.0, <span class="m">0</span>          <span class="c1"># First argument: adversarial loss feedback coefficient (0 to disable). Second argument: epochs to progressively increase the adversarial loss coefficient (0 to disable).</span>
--clf_low_loss_schedule 0.005, <span class="m">0</span>    <span class="c1"># First argument: low-frequency classifier loss feedback coefficient (0 to disable). Second argument: epochs to progressively increase the pixel loss coefficient (0 to disable).</span>
--clf_high_loss_schedule 0.005, <span class="m">0</span>   <span class="c1"># First argument: high-frequency classifier loss feedback coefficient (0 to disable). Second argument: epochs to progressively increase the pixel loss coefficient (0 to disable).</span>
--batch_size <span class="m">128</span>                    <span class="c1"># Batch size (training)</span>
--test_batch_size <span class="m">16</span>                <span class="c1"># Batch size (validation)</span>
--rnd_crops False                   <span class="c1"># Extract patches randomly (True) or from a non-overlapping grid (False)</span>
--n_epochs <span class="m">200</span>                      <span class="c1"># Total number of epochs</span>
--n_samples_per_epoch <span class="m">150000</span>        <span class="c1"># Number of training samples per epoch</span>
--gen_optimizer adam,lr<span class="o">=</span>0.0002      <span class="c1"># Generator optimizer (adam,lr=0.0002)</span>
--dis_optimizer adam,lr<span class="o">=</span>0.0002      <span class="c1"># Discriminator optimizer (adam,lr=0.0002)</span>
--save_opt True                     <span class="c1"># Save optimizer</span>
--lr_milestones <span class="o">[]</span>                  <span class="c1"># Epochs to divide learning rate by 10</span>

<span class="c1"># visualization parameters</span>
--padding <span class="m">20</span>                        <span class="c1"># Amount of padding (in pixels) between images in a single plot</span>

<span class="c1"># loaders / gpus</span>
--n_workers <span class="m">8</span>                       <span class="c1"># Number of workers per data loader</span>
--pin_memory True                   <span class="c1"># Pin memory of data loaders</span>
--gpu_devices <span class="m">0</span>                     <span class="c1"># Which gpu devices to use</span>

<span class="c1"># reload</span>
--reload <span class="s2">&quot;&quot;</span>                         <span class="c1"># Path to a pre-trained conditional GAN (and optimizer if saved)</span>
--clf_low_reload <span class="s2">&quot;&quot;</span>                 <span class="c1"># Path to a pre-trained low-frequency classifier (trained with classifier.py)</span>
--clf_high_reload <span class="s2">&quot;&quot;</span>                <span class="c1"># Path to a pre-trained high-frequency classifier (trained with classifier.py)</span>
--est_reload <span class="s2">&quot;&quot;</span>                     <span class="c1"># Path to a a pre-trained PRNU estimator (trained with estimator.py)</span>
--resume False                      <span class="c1"># Resume training</span>

<span class="c1"># debug</span>
--debug False                       <span class="c1"># Debug mode (only use a subset of the available data)</span>
</pre></div>

<h2 id="testing">Testing<a class="headerlink" href="#testing" title="Permanent link"></a></h2>
<p>Given a trained Cama model, to test its anonymization ability (attack success rates and distortion) on in-distribution or out-of-distribution test data run <code>python test.py</code>. Note that you must provide a path to either a low- or high-frequency non-interactive black-box target classifiers, which can be trained using <code>classifier.py</code>. See below for a full list of possible parameters:</p>
<div class="codehilite"><pre>python test.py

<span class="c1"># main parameters</span>
--ptc_fm <span class="m">3</span>                      <span class="c1"># Number of input feature maps (channels)</span>

<span class="c1"># testing parameters</span>
--test_batch_size <span class="m">16</span>            <span class="c1"># Batch size (testing)</span>
--in_dist True                  <span class="c1"># Are the test images in-distribution, i.e. captured by camera models known to conditional GAN?</span>
--comp_distortion True          <span class="c1"># Compute the distortion of the generator&#39;s outputs?</span>
--quantize False                <span class="c1"># Perform quantization?</span>
--save_transformed_imgs False   <span class="c1"># Save the transformed images to disk for reuse during other runs of testing?</span>

<span class="c1"># visualization parameters</span>
--visualize <span class="m">10</span>                  <span class="c1"># Number of transformation visualizations to save (0 to disable)</span>
--padding <span class="m">20</span>                    <span class="c1"># Amount of padding (in pixels) between images in a single plot</span>

<span class="c1"># loaders / gpus</span>
--n_workers <span class="m">8</span>                   <span class="c1"># Number of workers per data loader</span>
--pin_memory True               <span class="c1"># Pin memory of data loaders</span>
--gpu_devices <span class="m">0</span>                 <span class="c1"># Which gpu devices to use</span>

<span class="c1"># reload</span>
--reload <span class="s2">&quot;&quot;</span>                     <span class="c1"># Path to a pre-trained conditional GAN (trained with train.py)</span>
--clf_low_reload <span class="s2">&quot;&quot;</span>             <span class="c1"># Path to a pre-trained low-frequency classifier (trained with classifier.py)</span>
--clf_high_reload <span class="s2">&quot;&quot;</span>            <span class="c1"># Path to a pre-trained high-frequency classifier (trained with classifier.py)</span>
--est_reload <span class="s2">&quot;&quot;</span>                 <span class="c1"># Path to a a pre-trained PRNU estimator (trained with estimator.py)</span>
--transformed_imgs_reload <span class="s2">&quot;&quot;</span>    <span class="c1"># Path to pre-computed transformed images &#39;.pth&#39; file</span>
</pre></div>

<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link"></a></h2>
<p>Andrews, J.T.A., Zhang, Y. and Griffin, L.D., 2020. <a href="https://arxiv.org/abs/2002.07798">Conditional Adversarial Camera Model Anonymization</a>. <em>Proceedings of the European Conference on Computer Vision (ECCV) Workshops</em>.</p>
<div class="codehilite"><pre>@InProceedings{Andrews_2020_ECCV_Workshops},
author = {Andrews, Jerone T A and Zhang, Yidan and Griffin, Lewis D},
title = {Conditional Adversarial Camera Model Anonymization},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
year = {2020}
}
</pre></div>

<p>Contact: <a href="mailto:jerone.andrews@cs.ucl.ac.uk">jerone.andrews@cs.ucl.ac.uk</a></p></article></body></html>